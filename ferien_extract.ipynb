{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6152db17d3d7c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:06:35.792597Z",
     "start_time": "2024-05-06T13:06:35.535616Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edc497efa8a9479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:06:35.800596Z",
     "start_time": "2024-05-06T13:06:35.795610Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# URL der Webseite, von der wir die PDFs herunterladen möchten\n",
    "url = 'https://www.kmk.org/service/ferien/archiv-der-ferientermine.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2e577f1df0c63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:06:35.812279Z",
     "start_time": "2024-05-06T13:06:35.806611Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ordner erstellen, falls noch nicht vorhanden\n",
    "folder_path = './data/ferien'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e38f300fe970558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:06:35.822011Z",
     "start_time": "2024-05-06T13:06:35.815292Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def download_pdf(link, filename):\n",
    "    response = requests.get(link)\n",
    "    with open(os.path.join(folder_path, filename), 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f'Download completed: {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf04b5-a176-4c4c-b441-10de3881e211",
   "metadata": {},
   "source": [
    "# Nur bei Bedarf downloaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:06:57.379893Z",
     "start_time": "2024-05-06T13:06:35.825023Z"
    }
   },
   "outputs": [],
   "source": [
    "# # HTTP-Anfrage senden und Antwort bekommen\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# # Alle Links finden, die auf PDFs verweisen\n",
    "# for link in soup.find_all('a', href=True):\n",
    "#     href = link['href']\n",
    "#     if href.endswith('.pdf'):\n",
    "#         pdf_url = f'https://www.kmk.org{href}' if not href.startswith('http') else href\n",
    "#         filename = href.split('/')[-1]\n",
    "#         download_pdf(pdf_url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b464b98dd33c5de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:11:45.932671Z",
     "start_time": "2024-05-06T13:11:45.914567Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1994)\n",
      "Scraping 1995)\n",
      "Scraping 1996)\n",
      "Scraping 1997)\n",
      "Scraping 1998)\n",
      "Scraping 1999)\n",
      "Scraping 2000)\n",
      "Scraping 2001)\n",
      "Scraping 2002)\n",
      "Scraping 2003)\n",
      "Scraping 2004)\n",
      "Scraping 2005)\n",
      "Scraping 2006)\n",
      "Scraping 2007)\n",
      "Scraping 2008)\n",
      "Scraping 2009)\n",
      "Scraping 2010)\n",
      "Scraping 2011)\n",
      "Scraping 2012)\n",
      "Scraping 2013)\n",
      "Scraping 2014)\n",
      "Scraping 2015)\n",
      "Scraping 2016)\n",
      "Scraping 2017)\n",
      "Scraping 2018)\n",
      "Scraping 2019)\n",
      "Scraping 2020)\n",
      "Scraping 2021)\n",
      "Scraping 2022)\n",
      "Scraping 2023)\n",
      "Scraping 2024)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL der Webseite, von der du die Daten extrahieren möchtest\n",
    "url = \"https://www.schulferien.org/deutschland/ferien\"\n",
    "\n",
    "\n",
    "relevante_jahre = range(1994,2025)\n",
    "all_data = []\n",
    "for jahr in relevante_jahre:\n",
    "    url = f\"https://www.schulferien.org/deutschland/ferien/{jahr}/\"\n",
    "    print(f\"Scraping {jahr})\")\n",
    "    # HTTP-Anfrage an die URL senden\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Sicherstellen, dass die Anfrage erfolgreich war\n",
    "    \n",
    "    # HTML-Inhalt der Seite mit BeautifulSoup analysieren\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Die Tabelle finden, die die Feriendaten enthält\n",
    "    table = soup.find('table', class_='sf_table sf_table_responsive_block')\n",
    "    \n",
    "    # Die Daten aus der Tabelle extrahieren\n",
    "    data = []\n",
    "    headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        cols = [td.text.strip() for td in row.find_all('td')]\n",
    "        cols.append(jahr)\n",
    "        data.append(cols)\n",
    "    all_data.append(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc98be4edef5aef4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:06:57.435935Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#the first level of the list is the year, this is not necessary anymore so we flatten the list\n",
    "all_data_flat = [item for sublist in all_data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5276de70c7201aeb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:06:57.436935Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#remove any whitespace in every data in all_data_flat\n",
    "for row in all_data_flat:\n",
    "    for entry in row:\n",
    "        if isinstance(entry, str):\n",
    "            entry = entry.strip(\"*\\n \")\n",
    "            entry = entry.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deec471e2315bf1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:06:57.440935Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#remove any list in all_data_flat that has only 2 elements\n",
    "all_data_flat = [row for row in all_data_flat if len(row) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb64ca0-ef94-4d04-9faf-4ca3eccc961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [[s.strip().replace(\" \", \"\") if isinstance(s, str) else s for s in sublist] for sublist in all_data_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce43c0b-aeb9-4e2e-b1e2-937fd1e93020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the cleaning function to preserve both dashes \"-\" and dots \".\" in the data\n",
    "\n",
    "def clean_string_preserve_dashes_and_dots(s):\n",
    "    # Strip leading/trailing whitespace and newlines, remove leading/trailing non-alphanumeric characters\n",
    "    # except for dashes and dots which are preserved\n",
    "    return ''.join(c if c.isalnum() or c in {'-', '.'} else '' for c in s.strip())\n",
    "\n",
    "# Apply the revised cleaning function to all strings in the list\n",
    "cleaned_data_preserve_dashes_and_dots = [[clean_string_preserve_dashes_and_dots(s) if isinstance(s, str) else s for s in sublist] for sublist in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21295ac-f31b-4317-a0ce-d24dc0b6b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the regex patterns\n",
    "date_regex = re.compile(r'(\\d{2}\\.\\d{2}\\.-\\d{2}\\.\\d{2}\\.)')\n",
    "date_single = re.compile(r'(\\d{2}\\.\\d{2}\\.)')\n",
    "\n",
    "# Define a function to filter and clean the data based on the regex\n",
    "def filter_dates(data):\n",
    "    cleaned_list = []\n",
    "    for sublist in data:\n",
    "        new_sublist = []\n",
    "        for item in sublist:\n",
    "            if isinstance(item, str):\n",
    "                # Check if item matches date range pattern\n",
    "                if date_regex.search(item):\n",
    "                    new_sublist.append(item)\n",
    "                elif date_single.search(item):  # Check for single date\n",
    "                    new_sublist.append(item)\n",
    "                elif item == '-':  # Preserve single dash\n",
    "                    new_sublist.append(item)\n",
    "                # Preserve the state name if it does not match any date patterns\n",
    "                elif len(new_sublist) == 0:  \n",
    "                    new_sublist.append(item)\n",
    "            else:\n",
    "                new_sublist.append(item)\n",
    "        cleaned_list.append(new_sublist)\n",
    "    return cleaned_list\n",
    "\n",
    "# Apply the filter function to the cleaned data\n",
    "filtered_data = filter_dates(cleaned_data_preserve_dashes_and_dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "353cb129dbb1daa6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Die extrahierten Daten in einen Pandas DataFrame umwandeln\n",
    "columns = ['Bundesland', 'Winterferien', 'Osterferien', 'Pfingstferien', 'Sommerferien', 'Herbstferien', 'Weihnachtsferien', 'Jahr']\n",
    "df = pd.DataFrame(filtered_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008dbf97-4847-42c5-b8a0-e7e5fc0a250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate and correct date ranges\n",
    "import re\n",
    "\n",
    "def correct_date_ranges(date_str):\n",
    "    if pd.isna(date_str) or date_str.strip() == '-':\n",
    "        return date_str\n",
    "    match = re.match(r'^\\d{2}\\.\\d{2}\\.\\-\\d{2}\\.\\d{2}\\.$', date_str)\n",
    "    if match:\n",
    "        return date_str\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "for column in df.columns[2:-1]:  # excluding the first two columns and the year column\n",
    "    df[column] = df[column].apply(correct_date_ranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c3f325d-d1e9-4a4c-81b5-d3740cd85e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle the split and maintain '-' for both start and end if the original value is '-'\n",
    "def split_vacation_periods(df, column_name):\n",
    "    start_col = column_name + ' Start'\n",
    "    end_col = column_name + ' Ende'\n",
    "    df[[start_col, end_col]] = df[column_name].apply(lambda x: '-' if x == '-' else x).str.split('-', expand=True)\n",
    "    return df\n",
    "\n",
    "# List of vacation periods to split\n",
    "vacation_periods = ['Winterferien', 'Osterferien', 'Pfingstferien', 'Sommerferien', 'Herbstferien', 'Weihnachtsferien']\n",
    "\n",
    "# Applying the function to each vacation period\n",
    "for period in vacation_periods:\n",
    "    df = split_vacation_periods(df, period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be3b9d1-0d6b-4f70-9d67-b3ce74ee4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c730500a1dd6e0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:06:57.469007Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We have to transform the date ranges into a format that can be used by pandas\n",
    "# We add the year to the date ranges, from dd.mm. to dd.mm.yyyy\n",
    "df['Winterferien Start'] = df['Winterferien Start'] + df['Jahr'].astype(str)\n",
    "df['Winterferien Ende'] = df['Winterferien Ende']  + df['Jahr'].astype(str)\n",
    "df['Osterferien Start'] = df['Osterferien Start'] + df['Jahr'].astype(str)\n",
    "df['Osterferien Ende'] = df['Osterferien Ende']  + df['Jahr'].astype(str)\n",
    "df['Pfingstferien Start'] = df['Pfingstferien Start']  + df['Jahr'].astype(str)\n",
    "df['Pfingstferien Ende'] = df['Pfingstferien Ende']  + df['Jahr'].astype(str)\n",
    "df['Sommerferien Start'] = df['Sommerferien Start']  + df['Jahr'].astype(str)\n",
    "df['Sommerferien Ende'] = df['Sommerferien Ende']  + df['Jahr'].astype(str)\n",
    "df['Herbstferien Start'] = df['Herbstferien Start']  + df['Jahr'].astype(str)\n",
    "df['Herbstferien Ende'] = df['Herbstferien Ende']  + df['Jahr'].astype(str)\n",
    "df['Weihnachtsferien Start'] = df['Weihnachtsferien Start']  + df['Jahr'].astype(str)\n",
    "df['Weihnachtsferien Ende'] = df['Weihnachtsferien Ende']  + df['Jahr'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a4216f2-1522-40e3-b0dc-40330ffb18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c945a9a2-0da8-4ebb-9cac-8a09d6df4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date ranges into datetime objects for all specified columns\n",
    "df['Winterferien Start'] = pd.to_datetime(df['Winterferien Start'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Winterferien Ende'] = pd.to_datetime(df['Winterferien Ende'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Osterferien Start'] = pd.to_datetime(df['Osterferien Start'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Osterferien Ende'] = pd.to_datetime(df['Osterferien Ende'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Pfingstferien Start'] = pd.to_datetime(df['Pfingstferien Start'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Pfingstferien Ende'] = pd.to_datetime(df['Pfingstferien Ende'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Sommerferien Start'] = pd.to_datetime(df['Sommerferien Start'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Sommerferien Ende'] = pd.to_datetime(df['Sommerferien Ende'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Herbstferien Start'] = pd.to_datetime(df['Herbstferien Start'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Herbstferien Ende'] = pd.to_datetime(df['Herbstferien Ende'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Weihnachtsferien Start'] = pd.to_datetime(df['Weihnachtsferien Start'], format='%d.%m.%Y', errors='coerce')\n",
    "df['Weihnachtsferien Ende'] = pd.to_datetime(df['Weihnachtsferien Ende'], format='%d.%m.%Y', errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e46e03ecb55bcfb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T11:07:55.810945Z",
     "start_time": "2024-05-07T11:07:55.791033Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Add df to postresql\n",
    "from os import getenv\n",
    "from sqlalchemy import create_engine\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6a26b43ff69fa0f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define database connection parameters\n",
    "username = getenv('DB_USER').lower()\n",
    "password = getenv('DB_PASSWORD')\n",
    "host = getenv('DB_HOST')\n",
    "port = getenv('DB_PORT')\n",
    "database = getenv('DB_NAME')\n",
    "\n",
    "# Define the connection string\n",
    "# Format: dialect+driver://username:password@host:port/database\n",
    "connection_string = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c93715ed94f8f9b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-06T13:06:57.481005Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the DataFrame into the database\n",
    "df.to_sql('ferien', engine, schema='original_data', if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
